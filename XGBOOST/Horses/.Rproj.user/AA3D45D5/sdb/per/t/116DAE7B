{
    "collab_server" : "",
    "contents" : "### XGBoost\n# https://www.kaggle.com/rtatman/machine-learning-with-xgboost-in-r/\n\n# Install all needed packages in one operation, and then be smart.\n###\n\npackages = c(\"xgboost\",\"tidyverse\",\"DiagrammeR\")\n\npackage.check <- lapply(packages, FUN = function(pkg) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg, dependencies = TRUE)\n    library(pkg, character.only = TRUE)\n  }\n})\n\n## reading the input file \n\ndiseaseInfo <-read.csv(\"Outbreak_240817.csv\")\n\n\n\n\n# testing and training using sampling \nset.seed(1234)\n\ndiseaseInfo <- diseaseInfo[sample(1:nrow(diseaseInfo)),]\n\n## removing info about target variable \n\ndiseaseInfo_humansRemoved <- diseaseInfo %>%\n  select(-starts_with(\"human\"))\n\n## getting labels \n\ndiseaseLabels <- diseaseInfo %>%\n  select(humansAffected) %>%  # to get column with # of humans affected\n  is.na() %>% # check if na\n  magrittr::not() # reverse the result of is.na, ie: false means: humans are affected but i need to change it to True.\n\n# reduce the redundant information \n# removing id, long lat(as country is already there), removing any non numeric features \n\ndiseaseInfo_numeric <- diseaseInfo_humansRemoved %>%\n  select(-Id) %>%\n  select(-c(longitude,latitude)) %>%\n  select_if(is.numeric)\n\n\n# converting categorial to numeric \n\n# one hot encoding for country \n\nregion <- model.matrix(~country-1,diseaseInfo)\n\nhead(region)\n\n# for species description \nsummary(diseaseInfo$speciesDescription)\n\ndiseaseInfo_numeric$is_domestic <- as.numeric(str_detect(diseaseInfo$speciesDescription,\"domestic\"))\n\n\n# now we are checking for species type which is stored in second str of species description\n\n\nspeciesList <- diseaseInfo$speciesDescription %>%\n  str_replace(\"[[:punct:]]\", \"\") %>% # remove punctuation (some rows have parentheses)\n  str_extract(\"[a-z]*$\") # extract the least word in each row\n\n# convert our list into a dataframe...\nspeciesList <- tibble(species = speciesList)\n\n# and convert to a matrix using 1 hot encoding\noptions(na.action='na.pass') # don't drop NA values!\nspecies <- model.matrix(~species-1,speciesList)\n\n\n\n#########\n\n# combining diseaseinfo_numeric, region and species together \n\ndiseaseInfo_numeric <- cbind(diseaseInfo_numeric,region,species)\n\nclass(region\n      )\n\n# converting into matrix as xgboost only works with matrix \ndiseaseInfo_matrix <- data.matrix(diseaseInfo_numeric)\n\n######### test and train ######\n\n# next step is to convert into test(30%) and train data(70%)\n\ntrain_len <- round(nrow(diseaseInfo_matrix)*0.7)\n\n# training data \ntrain_data <- diseaseInfo_matrix[1:train_len,]\ntrain_lables <- diseaseLabels[1:train_len,]\n\n# test data\ntest_data <- diseaseInfo_matrix[-(1:train_len),]\ntest_lables <- diseaseLabels[-(1:train_len),]\n\n########## converting data into dmatrix ######\n\ndtrain <- xgb.DMatrix(data=train_data, label = train_lables)\n\ndtest <- xgb.DMatrix(data=test_data, label=test_lables)\n\n\n########## Model Training ##########\n\nmodel <- xgboost(data=dtrain,\n                 nround=2,\n                 objective = 'binary:logistic')\n# so both models have same error, that means our second model is not an improvement on our 1st model \n\n########## checking performance for test data #### \n\npred <- predict(model,dtest)\n\n# get & print the classification error\nerr <- mean((pred > 0.5) != test_lables)\nprint(paste(\"test-error=\", err))\n\n\n### \"test-error= 0.0121520972167777\" this means that our model is not overfit as error is less than the error in train model\n\n######### Tuning our model \n\n#  overfitting \n#  we can reduce the depth of each tree by paramenet max.depth\n#  the default depth is 6\n\nmodel <- xgboost(data=dtrain,\n                 max.depth=3,\n                 nrounds=2,\n                 objective=\"binary:logistic\")\n\n# testing \n\npred <- predict(model, dtest)\n\nerr <- mean((pred >0.5) != test_lables)\nprint(paste(\"test-error\",err))\n\n# no improvement since there was no overfitting to start with\n\n############# Further tuning\n# 1. early stopping rounds : stop if there is no improvment after this many rounds\n# 2. scale_pos_weight : to take care of imbalanced class. so in our case most cases are false, so we need our ensemble model to have weights for each model accordingly \n# 3. Gamma: for regularization \n\nnegative_cases=sum(diseaseLabels == FALSE)\npositive_cases=sum(diseaseLabels == TRUE)\n\n\nmodel_tuned <- xgboost(data = dtrain, # the data           \n                       max.depth = 3, # the maximum depth of each decision tree\n                       nround = 10, # number of boosting rounds\n                       early_stopping_rounds = 3, # if we dont see an improvement in this many rounds, stop\n                       objective = \"binary:logistic\", # the objective function\n                       scale_pos_weight = negative_cases/positive_cases, # control for imbalanced classes\n                       gamma = 1) # add a regularization term\n\npred <- predict(model_tuned,dtest)\n\nerr <- mean((pred>0.5) != test_lables)\nprint(paste(\"test error after tuning :\", err))\n\n\n###### Examining the model ######\n\n# plot them features! what's contributing most to our model?\nxgb.plot.multi.trees(feature_names = names(diseaseInfo_numeric), \n                     model = model_tuned)\n\nimportance_matrix <- xgb.importance(names(diseaseInfo_numeric), model = model)\n\nnames(diseaseInfo_numeric)\n\n# and plot it!\nxgb.plot.importance(importance_matrix)\n",
    "created" : 1517528609882.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "671792733",
    "id" : "116DAE7B",
    "lastKnownWriteTime" : 1517527336,
    "last_content_update" : 1517527336,
    "path" : "~/Desktop/Kaggle/XGBOOST/xgboost.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}